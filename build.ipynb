{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall FPS: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import wandb  # For experiment tracking\n",
    "\n",
    "class MTCNNTrainer:\n",
    "    def __init__(self, model: CustomMTCNN, config: Dict, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.optimizers = {\n",
    "            'pnet': optim.Adam(model.pnet.parameters(), lr=config['learning_rate']),\n",
    "            'rnet': optim.Adam(model.rnet.parameters(), lr=config['learning_rate']),\n",
    "            'onet': optim.Adam(model.onet.parameters(), lr=config['learning_rate'])\n",
    "        }\n",
    "        \n",
    "        # Initialize learning rate schedulers\n",
    "        self.schedulers = {\n",
    "            name: optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',\n",
    "                patience=config['lr_patience'],\n",
    "                factor=0.1\n",
    "            )\n",
    "            for name, optimizer in self.optimizers.items()\n",
    "        }\n",
    "        \n",
    "        # Initialize loss function\n",
    "        self.criterion = MTCNNLoss(\n",
    "            det_weight=config['det_weight'],\n",
    "            box_weight=config['box_weight'],\n",
    "            landmark_weight=config['landmark_weight']\n",
    "        )\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger('MTCNNTrainer')\n",
    "        \n",
    "    def train_step(self, net_type: str, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass based on network type\n",
    "        if net_type == 'pnet':\n",
    "            det_pred, box_pred, landmark_pred = self.model.pnet(batch['image'])\n",
    "        elif net_type == 'rnet':\n",
    "            det_pred, box_pred, landmark_pred = self.model.rnet(batch['image'])\n",
    "        else:  # onet\n",
    "            det_pred, box_pred, landmark_pred = self.model.onet(batch['image'])\n",
    "        \n",
    "        # Compute losses\n",
    "        losses = self.criterion(\n",
    "            det_pred, box_pred, landmark_pred,\n",
    "            batch['det_target'], batch['box_target'], batch['landmark_target']\n",
    "        )\n",
    "        \n",
    "        # Optimization step\n",
    "        self.optimizers[net_type].zero_grad()\n",
    "        losses['total'].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.model.state_dict()[net_type].parameters(),\n",
    "            self.config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        self.optimizers[net_type].step()\n",
    "        \n",
    "        return {k: v.item() for k, v in losses.items()}\n",
    "    \n",
    "    def validate(self, net_type: str, val_loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"Validation step\"\"\"\n",
    "        self.model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                if net_type == 'pnet':\n",
    "                    det_pred, box_pred, landmark_pred = self.model.pnet(batch['image'])\n",
    "                elif net_type == 'rnet':\n",
    "                    det_pred, box_pred, landmark_pred = self.model.rnet(batch['image'])\n",
    "                else:  # onet\n",
    "                    det_pred, box_pred, landmark_pred = self.model.onet(batch['image'])\n",
    "                \n",
    "                losses = self.criterion(\n",
    "                    det_pred, box_pred, landmark_pred,\n",
    "                    batch['det_target'], batch['box_target'], batch['landmark_target']\n",
    "                )\n",
    "                val_losses.append({k: v.item() for k, v in losses.items()})\n",
    "        \n",
    "        # Compute average validation losses\n",
    "        avg_losses = {}\n",
    "        for key in val_losses[0].keys():\n",
    "            avg_losses[key] = sum(loss[key] for loss in val_losses) / len(val_losses)\n",
    "        \n",
    "        return avg_losses\n",
    "    \n",
    "    def train_network(self, net_type: str, train_loader: DataLoader, val_loader: DataLoader, num_epochs: int) -> None:\n",
    "        \"\"\"Train a single network (P-Net, R-Net, or O-Net)\"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "            \n",
    "            with tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
    "                for batch in pbar:\n",
    "                    step_losses = self.train_step(net_type, batch)\n",
    "                    train_losses.append(step_losses)\n",
    "                    pbar.set_postfix(loss=step_losses['total'])\n",
    "            \n",
    "            # Compute average training losses\n",
    "            avg_train_losses = {}\n",
    "            for key in train_losses[0].keys():\n",
    "                avg_train_losses[key] = sum(loss[key] for loss in train_losses) / len(train_losses)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_losses = self.validate(net_type, val_loader)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.schedulers[net_type].step(val_losses['total'])\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_losses['total'] < best_val_loss:\n",
    "                best_val_loss = val_losses['total']\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                self.save_checkpoint(net_type, epoch, val_losses['total'])\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Log metrics\n",
    "            metrics = {\n",
    "                f'{net_type}/train_{k}': v for k, v in avg_train_losses.items()\n",
    "            }\n",
    "            metrics.update({\n",
    "                f'{net_type}/val_{k}': v for k, v in val_losses.items()\n",
    "            })\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= self.config['early_stopping_patience']:\n",
    "                self.logger.info(f'Early stopping triggered for {net_type}')\n",
    "                break\n",
    "    \n",
    "    def train(self, train_loaders: Dict[str, DataLoader], val_loaders: Dict[str, DataLoader]) -> None:\n",
    "        \"\"\"Complete training procedure for all networks\"\"\"\n",
    "        # Initialize wandb\n",
    "        wandb.init(\n",
    "            project=\"mtcnn-training\",\n",
    "            config=self.config\n",
    "        )\n",
    "        \n",
    "        # Sequential training of networks\n",
    "        for net_type in ['pnet', 'rnet', 'onet']:\n",
    "            self.logger.info(f'Training {net_type}...')\n",
    "            self.train_network(\n",
    "                net_type,\n",
    "                train_loaders[net_type],\n",
    "                val_loaders[net_type],\n",
    "                self.config['num_epochs'][net_type]\n",
    "            )\n",
    "        \n",
    "        wandb.finish()\n",
    "    \n",
    "    def save_checkpoint(self,net_type: str, epoch: int, val_loss: float) -> None:\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizers[net_type].state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(\n",
    "            self.config['checkpoint_dir'],\n",
    "            f'{net_type}_checkpoint_epoch_{epoch}.pth'\n",
    "        )\n",
    "        torch.save(checkpoint, path)\n",
    "        self.logger.info(f'Checkpoint saved: {path}')\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': {\n",
    "        'pnet': 30,\n",
    "        'rnet': 20,\n",
    "        'onet': 20\n",
    "    },\n",
    "    'batch_size': 64,\n",
    "    'det_weight': 1.0,\n",
    "    'box_weight': 0.5,\n",
    "    'landmark_weight': 0.5,\n",
    "    'lr_patience': 3,\n",
    "    'early_stopping_patience': 5,\n",
    "    'max_grad_norm': 5.0,\n",
    "    'checkpoint_dir': './checkpoints'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
